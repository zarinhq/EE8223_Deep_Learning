{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "918c2cab",
      "metadata": {
        "id": "918c2cab"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to Google drive in order to store the weights of the agents HDF5 files.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGXOKY6zmXwU",
        "outputId": "ab1a10ba-2cf1-4236-dfa0-dfcd5955c526"
      },
      "id": "WGXOKY6zmXwU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "72f7194c",
      "metadata": {
        "id": "72f7194c"
      },
      "outputs": [],
      "source": [
        "seed_value = 42\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3bfe605",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3bfe605",
        "outputId": "8a920add-8956-4fc5-b35b-6ee0db20e3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.0-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Collecting biosppy\n",
            "  Downloading biosppy-1.0.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wfdb) (1.7.3)\n",
            "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wfdb) (1.3.5)\n",
            "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from wfdb) (0.11.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from wfdb) (3.2.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in /usr/local/lib/python3.8/dist-packages (from wfdb) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib<4.0.0,>=3.2.2->wfdb) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.24.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
            "Collecting shortuuid\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from biosppy) (1.2.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from biosppy) (4.6.0.66)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from biosppy) (1.0.2)\n",
            "Collecting bidict\n",
            "  Downloading bidict-0.22.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from biosppy) (3.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->biosppy) (3.1.0)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9674 sha256=43be420be5c74bd1fd560aa28064ac72f8f7b7974773ac5d882ae6435b9cfef4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
            "Successfully built wget\n",
            "Installing collected packages: shortuuid, bidict, wget, wfdb, biosppy\n",
            "Successfully installed bidict-0.22.0 biosppy-1.0.0 shortuuid-1.0.11 wfdb-4.1.0 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "pip install wfdb wget tqdm biosppy imbalanced-learn seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-determinism"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ8V5aOGFhvN",
        "outputId": "91be12f6-12e1-47ff-8bd1-b25d9a811008"
      },
      "id": "rQ8V5aOGFhvN",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-determinism\n",
            "  Downloading tensorflow-determinism-0.3.0.tar.gz (12 kB)\n",
            "Building wheels for collected packages: tensorflow-determinism\n",
            "  Building wheel for tensorflow-determinism (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-determinism: filename=tensorflow_determinism-0.3.0-py3-none-any.whl size=9155 sha256=0c3b3e7f00e6f69ad2426c4d4d9b796b5fa2fd203b5dd180208367319e83f99f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/5e/dd/9670c2d20867bcff6eb19199822f6b32f16bbe01bea1cd35a8\n",
            "Successfully built tensorflow-determinism\n",
            "Installing collected packages: tensorflow-determinism\n",
            "Successfully installed tensorflow-determinism-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6250356",
      "metadata": {
        "id": "c6250356"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "713c0a4c",
      "metadata": {
        "id": "713c0a4c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import wget\n",
        "import math\n",
        "import zipfile\n",
        "import wfdb as wf\n",
        "import pickle\n",
        "import sys\n",
        "import datetime\n",
        "import cv2\n",
        "import random\n",
        "import random as python_random\n",
        "from scipy import signal\n",
        "from scipy.signal import resample\n",
        "from scipy.signal import find_peaks\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bc66bc35",
      "metadata": {
        "id": "bc66bc35"
      },
      "outputs": [],
      "source": [
        "def reset_random_seeds():\n",
        "   os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "   os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "846dd319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "846dd319",
        "outputId": "e0fe9880-92e2-4de2-9c15-84392432d101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "0.6394267984578837\n",
            "tf.Tensor([0.6645621], shape=(1,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(seed_value)\n",
        "vec = np.random.randint(1, 10)\n",
        "print(vec)\n",
        "random.seed(42)\n",
        "print(random.random())\n",
        "tf.random.set_seed(42)\n",
        "print(tf.random.uniform([1])) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222ea7e0",
      "metadata": {
        "id": "222ea7e0"
      },
      "source": [
        "## Extracting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7d9ec058",
      "metadata": {
        "id": "7d9ec058"
      },
      "outputs": [],
      "source": [
        "module = os.path.abspath('./WESAD/')\n",
        "if module not in sys.path:\n",
        "  sys.path.append(module)\n",
        "from DataManager import DataManager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f16ff72e",
      "metadata": {
        "id": "f16ff72e"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bc630bdd",
      "metadata": {
        "id": "bc630bdd"
      },
      "outputs": [],
      "source": [
        "# config\n",
        "debug = False;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0d8d66af",
      "metadata": {
        "id": "0d8d66af"
      },
      "outputs": [],
      "source": [
        "# subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c1d8eff6",
      "metadata": {
        "id": "c1d8eff6"
      },
      "outputs": [],
      "source": [
        "test_subject = [3]\n",
        "rest_subjects = [2, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "66da8248",
      "metadata": {
        "id": "66da8248"
      },
      "outputs": [],
      "source": [
        "def generateData(subject_ids):\n",
        "    data = DataManager()\n",
        "    all_subject_data = np.array([]);\n",
        "    all_subject_labels = np.array([]);\n",
        "\n",
        "#     subject_ids = [2, 3]\n",
        "\n",
        "    for subject_id in subject_ids:\n",
        "        if(debug): print(\"Processing subject id: \", subject_id);\n",
        "        subject = data.load(subject_id)\n",
        "\n",
        "\n",
        "        if(debug):\n",
        "            print('baseline: ', subject[0], '\\nstress: ', subject[1], '\\namusement: ', subject[2], '\\nbase_label: ', subject[3], '\\nstress_label: ', subject[4], '\\namuse_label: ', subject[5])\n",
        "\n",
        "        ecg_base = subject[0]\n",
        "        ecg_stress = subject[1]\n",
        "        ecg_amusement = subject[2]\n",
        "\n",
        "        base_label = subject[3]\n",
        "        stress_label = subject[4]\n",
        "        amuse_label = subject[5]\n",
        "\n",
        "        ecg_base_T = ecg_base.T\n",
        "        ecg_stress_T = ecg_stress.T\n",
        "        ecg_amusement_T = ecg_amusement.T\n",
        "\n",
        "        ecg_base_data = ecg_base_T[0]\n",
        "        ecg_stress_data = ecg_stress_T[0]\n",
        "        ecg_amusement_data = ecg_amusement_T[0]\n",
        "\n",
        "        ecg_base_label =  base_label.T\n",
        "        ecg_stress_label  = stress_label.T\n",
        "        ecg_amusement_label  = amuse_label.T\n",
        "\n",
        "        if(debug):\n",
        "            print('ecg_base_data:', ecg_base_data)\n",
        "            print('ecg_stress_data:', ecg_stress_data)\n",
        "            print('ecg_amusement_data:', ecg_amusement_data)\n",
        "\n",
        "            print('ecg_base_label:', ecg_base_label)\n",
        "            print('ecg_stress_label:', ecg_stress_label)\n",
        "            print('ecg_amusement_label:', ecg_amusement_label)\n",
        "\n",
        "\n",
        "        base_secs = len(ecg_base_data) // 700 # Number of seconds in signal X\n",
        "        base_samps = int(base_secs * 256 )    # Number of samples to downsample\n",
        "        base_down = signal.resample(ecg_base_data, base_samps)\n",
        "\n",
        "\n",
        "        stress_secs = len(ecg_stress_data) // 700 # Number of seconds in signal X\n",
        "        stress_samps = int(stress_secs * 256 )    # Number of samples to downsample\n",
        "        stress_down = signal.resample(ecg_stress_data, stress_samps)\n",
        "\n",
        "\n",
        "        amuse_secs = len(ecg_amusement_data) // 700 # Number of seconds in signal X\n",
        "        amuse_samps = int(amuse_secs * 256 )    # Number of samples to downsample\n",
        "        amuse_down = signal.resample(ecg_amusement_data, amuse_samps)\n",
        "\n",
        "    #     heart_beat_base = np.array_split(base_down, 256)\n",
        "    #     heart_beat_stress = np.array_split(stress_down, 256)\n",
        "    #     heart_beat_amuse = np.array_split(amuse_down, 256)\n",
        "\n",
        "        window_size = 256\n",
        "        window_shift = 256\n",
        "\n",
        "        heart_beat_base = []\n",
        "        for i in range(0,len(base_down) - window_size,window_shift):\n",
        "            heart_beat_base.append(base_down[i:window_size + i])\n",
        "        heart_beat_base.pop()\n",
        "    #     heart_beat_base = np.array(list(heart_beat_base[:]), dtype=float)\n",
        "\n",
        "        heart_beat_stress = []\n",
        "        for i in range(0,len(stress_down) - window_size,window_shift):\n",
        "            heart_beat_stress.append(stress_down[i:window_size + i])\n",
        "        heart_beat_stress.pop()\n",
        "    #     heart_beat_stress = np.array(list(heart_beat_stress[:]), dtype=float)\n",
        "\n",
        "        heart_beat_amuse = []\n",
        "        for i in range(0,len(amuse_down) - window_size,window_shift):\n",
        "            heart_beat_amuse.append(amuse_down[i:window_size + i])\n",
        "        heart_beat_amuse.pop()\n",
        "    #     heart_beat_amuse = np.array(list(heart_beat_amuse[:]), dtype=float)\n",
        "\n",
        "\n",
        "        for idx, idxval in enumerate(heart_beat_base):\n",
        "            heart_beat_base[idx] = (heart_beat_base[idx] - heart_beat_base[idx].min()) / heart_beat_base[idx].ptp() # Normalize the readings to a 0-1 range \n",
        "            heart_beat_base[idx] = np.append(heart_beat_base[idx], 0.0) #Baseline = 0\n",
        "\n",
        "        for idx, idxval in enumerate(heart_beat_stress):\n",
        "            heart_beat_stress[idx] = (heart_beat_stress[idx] - heart_beat_stress[idx].min()) / heart_beat_stress[idx].ptp() # Normalize the readings to a 0-1 range \n",
        "            heart_beat_stress[idx] = np.append(heart_beat_stress[idx], 1.0) #Stress = 1    \n",
        "\n",
        "        for idx, idxval in enumerate(heart_beat_amuse):\n",
        "            heart_beat_amuse[idx] = (heart_beat_amuse[idx] - heart_beat_amuse[idx].min()) / heart_beat_amuse[idx].ptp() # Normalize the readings to a 0-1 range \n",
        "            heart_beat_amuse[idx] = np.append(heart_beat_amuse[idx], 2.0) #Amusement = 2\n",
        "\n",
        "    #     print(\"heart_beat_base:\", len(heart_beat_base[0]), len(heart_beat_base[-2]), len(heart_beat_base[-1]))\n",
        "    #     print(\"heart_beat_stress:\", len(heart_beat_stress[0]), len(heart_beat_stress[-1]))\n",
        "    #     print(\"heart_beat_amuse:\", len(heart_beat_amuse[0]), len(heart_beat_amuse[-1]))\n",
        "\n",
        "        heart_beat_all = np.concatenate((heart_beat_base, heart_beat_stress, heart_beat_amuse), axis=0)\n",
        "        subject_data = np.array(list(heart_beat_all[:]), dtype=float)\n",
        "\n",
        "\n",
        "        if(all_subject_data.size == 0):\n",
        "            all_subject_data = subject_data\n",
        "        else:\n",
        "            if(subject_data.size != 0):\n",
        "                all_subject_data = np.concatenate((all_subject_data, subject_data), axis=0)\n",
        "\n",
        "        print(\"New data shape\", subject_data.shape, \"Total Shape: \", all_subject_data.shape)\n",
        "        \n",
        "    #SMOTE to balance the data\n",
        "    df_final_data_X = pd.DataFrame(data=all_subject_data[:, :-1])\n",
        "    df_final_data_Y = pd.DataFrame(data=all_subject_data[:,-1])\n",
        "\n",
        "    smote = SMOTE(sampling_strategy='not majority')\n",
        "    X_sm, y_sm = smote.fit_resample(df_final_data_X, df_final_data_Y)\n",
        "    \n",
        "    return X_sm, y_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8232f1fa",
      "metadata": {
        "id": "8232f1fa"
      },
      "source": [
        "## Data Generation - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "78e15cc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e15cc1",
        "outputId": "09bbaa6f-0c89-4b8b-99d2-cee9dc3b7dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for S2\n",
            "New data shape (2115, 257) Total Shape:  (2115, 257)\n",
            "Loading data for S4\n",
            "New data shape (2159, 257) Total Shape:  (4274, 257)\n",
            "Loading data for S5\n",
            "New data shape (2211, 257) Total Shape:  (6485, 257)\n",
            "Loading data for S6\n",
            "New data shape (2196, 257) Total Shape:  (8681, 257)\n",
            "Loading data for S7\n",
            "New data shape (2192, 257) Total Shape:  (10873, 257)\n",
            "Loading data for S8\n",
            "New data shape (2202, 257) Total Shape:  (13075, 257)\n",
            "Loading data for S9\n",
            "New data shape (2191, 257) Total Shape:  (15266, 257)\n",
            "Loading data for S10\n",
            "New data shape (2271, 257) Total Shape:  (17537, 257)\n",
            "Loading data for S11\n",
            "New data shape (2222, 257) Total Shape:  (19759, 257)\n",
            "Loading data for S13\n",
            "New data shape (2220, 257) Total Shape:  (21979, 257)\n",
            "Loading data for S14\n",
            "New data shape (2221, 257) Total Shape:  (24200, 257)\n",
            "Loading data for S15\n",
            "New data shape (2227, 257) Total Shape:  (26427, 257)\n",
            "Loading data for S16\n",
            "New data shape (2215, 257) Total Shape:  (28642, 257)\n",
            "Loading data for S17\n",
            "New data shape (2270, 257) Total Shape:  (30912, 257)\n"
          ]
        }
      ],
      "source": [
        "train_data, train_labels = generateData(rest_subjects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c46ed6db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46ed6db",
        "outputId": "be1db41a-8b57-4354-d1b5-4e94f2500bfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    16443\n",
              "1.0    16443\n",
              "2.0    16443\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "480c46f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480c46f0",
        "outputId": "45fe217e-6871-4a9c-a025-259aa0709b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (49329, 256)\n",
            "Train labels shape: (49329, 1)\n",
            "Dimension: 2\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Dimension:\", train_labels.ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "71b2fcf4",
      "metadata": {
        "id": "71b2fcf4"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.DataFrame(train_labels).to_numpy()\n",
        "train_labels = train_labels.reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "62a1973a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62a1973a",
        "outputId": "9f86b780-6d5d-4702-bd9b-5813e210a44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels shape: (49329,)\n",
            "Dimension: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Dimension:\", train_labels.ndim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e728c1",
      "metadata": {
        "id": "76e728c1"
      },
      "source": [
        "## Train and Validation data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5d6b046b",
      "metadata": {
        "id": "5d6b046b"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b575ded6",
      "metadata": {
        "id": "b575ded6"
      },
      "outputs": [],
      "source": [
        "#Reshape train and validation data to (n_samples, 256, 1), where each sample is of size (256, 1)\n",
        "X_train = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val = np.array(X_val).reshape(X_val.shape[0], X_val.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f2c4f94a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2c4f94a",
        "outputId": "d56fb892-bd6a-40f3-bdb9-0a08ba85d5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of X_train: (34530, 256, 1)\n",
            "size of X_val: (14799, 256, 1)\n",
            "size of y_train: (34530,)\n",
            "size of y_val: (14799,)\n",
            "Train: Counter({2.0: 11546, 0.0: 11500, 1.0: 11484}) Counter({1.0: 4959, 0.0: 4943, 2.0: 4897})\n"
          ]
        }
      ],
      "source": [
        "print('size of X_train:', X_train.shape)\n",
        "print('size of X_val:', X_val.shape)\n",
        "\n",
        "print('size of y_train:', y_train.shape)\n",
        "print('size of y_val:', y_val.shape)\n",
        "\n",
        "print('Train:', Counter(y_train), Counter(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527eb4b4",
      "metadata": {
        "id": "527eb4b4"
      },
      "source": [
        "## Data Generation - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a30c80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91a30c80",
        "outputId": "1c00d2e1-1888-40f0-c945-a5712d9e7e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for S3\n"
          ]
        }
      ],
      "source": [
        "test_data, test_labels = generateData(test_subject)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c08e23de",
      "metadata": {
        "id": "c08e23de"
      },
      "outputs": [],
      "source": [
        "test_labels.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d493ea",
      "metadata": {
        "id": "95d493ea"
      },
      "outputs": [],
      "source": [
        "print(\"Test data shape:\", test_data.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n",
        "print(\"Dimension:\", test_labels.ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f261521",
      "metadata": {
        "id": "7f261521"
      },
      "outputs": [],
      "source": [
        "test_labels = pd.DataFrame(test_labels).to_numpy()\n",
        "y_test = test_labels.reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf6621a",
      "metadata": {
        "id": "eaf6621a"
      },
      "outputs": [],
      "source": [
        "print(\"Test labels shape:\", y_test.shape)\n",
        "print(\"Dimension:\", y_test.ndim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef372ad9",
      "metadata": {
        "id": "ef372ad9"
      },
      "outputs": [],
      "source": [
        "#Reshape test data to (n_samples, 256, 1), where each sample is of size (256, 1)\n",
        "X_test = np.array(test_data).reshape(test_data.shape[0], test_data.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7178055",
      "metadata": {
        "id": "f7178055"
      },
      "outputs": [],
      "source": [
        "print('size of X_test:', X_test.shape)\n",
        "print('size of y_test:', y_test.shape)\n",
        "\n",
        "print('Test:', Counter(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39a597b",
      "metadata": {
        "id": "a39a597b"
      },
      "source": [
        "## LSTM Model Architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e7d81d",
      "metadata": {
        "id": "a9e7d81d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, BatchNormalization, MaxPool1D, Bidirectional, LSTM, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5805ab",
      "metadata": {
        "id": "3d5805ab"
      },
      "outputs": [],
      "source": [
        "reset_random_seeds()\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Bidirectional(LSTM(64, input_shape= (X_train.shape[1],1), return_sequences = True)))\n",
        "lstm_model.add(Bidirectional(LSTM(32)))\n",
        "lstm_model.add(Flatten())\n",
        "lstm_model.add(Dense(units = 128, activation='relu'))\n",
        "lstm_model.add(Dense(units = 3, activation='softmax'))\n",
        "\n",
        "lstm_model.compile(optimizer= 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9bdf53",
      "metadata": {
        "id": "7f9bdf53"
      },
      "outputs": [],
      "source": [
        "epochs=50\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfea2b2",
      "metadata": {
        "id": "fdfea2b2"
      },
      "outputs": [],
      "source": [
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.005\n",
        "  drop = 0.6\n",
        "  epochs_drop = 10.0\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callbacks_list = [lrate]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4866fedc",
      "metadata": {
        "id": "4866fedc"
      },
      "outputs": [],
      "source": [
        "history = lstm_model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = (X_val, y_val), callbacks=callbacks_list, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c22a2bc",
      "metadata": {
        "id": "3c22a2bc"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "t = f.suptitle('1D CNN Performance', fontsize=12)\n",
        "f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "\n",
        "max_epoch = len(history.history['accuracy'])+1\n",
        "epoch_list = list(range(1,max_epoch))\n",
        "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
        "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
        "ax1.set_ylabel('Accuracy Value')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_title('Accuracy')\n",
        "l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
        "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
        "ax2.set_ylabel('Loss Value')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_title('Loss')\n",
        "l2 = ax2.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29da2d94",
      "metadata": {
        "id": "29da2d94"
      },
      "outputs": [],
      "source": [
        "lstm_model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_test_preds = np.around(lstm_model.predict(X_test))\n",
        "print(y_test_preds)"
      ],
      "metadata": {
        "id": "-Z8JoNrctmN0"
      },
      "id": "-Z8JoNrctmN0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = np.argmax(y_test_preds, axis=1)"
      ],
      "metadata": {
        "id": "scBaMxWkQSj3"
      },
      "id": "scBaMxWkQSj3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics"
      ],
      "metadata": {
        "id": "T58iPD8RQYcJ"
      },
      "id": "T58iPD8RQYcJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "id": "bQHhS5H9vY1S"
      },
      "id": "bQHhS5H9vY1S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7LicsW_t2cyv"
      },
      "id": "7LicsW_t2cyv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}